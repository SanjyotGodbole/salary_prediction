---
title: "Regression"
author: "Sanjyot Godbole"
date: "February 12, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Clearing the environment
```{r ClearingEnvironment}
rm(list = ls(all=TRUE))
```

##Loading the dataset and creating safe copies to retreive the original data in case of excessive data loss
```{r}

Train_Data = read.csv("train.csv", header = T,sep = ",")
dataToPredict = read.csv("test.csv", header = T,sep = ",")
dim(Train_Data)
dim(dataToPredict)

```

##Descriptive Analysis of Train_Data
```{r}
# str(Train_Data)
# head(Train_Data)
# tail(Train_Data)
# summary(Train_Data)
# names(Train_Data)
```


##Handling missing values
### Train Dataset
**Using KNN Imputation**
```{r}
#Checking dimensions before handling missing values
dim(Train_Data)

#Checking total number of missing values in the dataset
sum(is.na(Train_Data))

#Checking feature-wise missing values
for (feature in names(Train_Data)) {
    missing <- sum(is.na(Train_Data[,feature]))
    if (missing > 0) {
        print(c(feature,missing))
    }
}

#To identify rows where more than 20% attributes are missing
library(DMwR)
manyNAs(Train_Data, nORp = 0.2) 
?manyNAs

#Removes the whole rows containing NA
#Train_Data=na.omit(Train_Data)         #Removes the whole row. Might result in high data loss


#Assigning NA to missing values which are not NA
Train_Data[Train_Data=="?"]<-NA                    #Assigning NA to missing values
Train_Data[Train_Data=="?"]<-NA

#########################################################################################

#Removing features having huge number of missing values
sum(is.na(Train_Data$featurename))
Train_Data$featurename = NULL # Number of missing values for featurename = #####

#Checking the number of missing values left after removing the above feature/features
sum(is.na(financialData))


##########################################################################################

#Imputting remanining values using KNNimputation
library(DMwR)
# 
Train_Data_no_NA<-knnImputation(Train_Data)
#View(Train_Data_no_NA)

#Checking the number of missing values left after KNN imputation
sum(is.na(Train_Data_no_NA))
dim(Train_Data_no_NA)

#########################################################################################

#Saving a CSV file of imputed data to avoid imputation time while re-running the code  
#write.csv(Train_Data_no_NA, "Train_Data_no_NA_new.csv", row.names=F)

#Train_Data_no_NA = read.csv("Train_Data_no_NA_new.csv", header = T,sep = ",")
```

### dataToPredict Dataset
**Using KNN Imputation**
```{r}
#Checking dimensions before handling missing values
dim(dataToPredict)

#Checking total number of missing values in the dataset
sum(is.na(dataToPredict))

#Checking feature-wise missing values
for (feature in names(dataToPredict)) {
    missing <- sum(is.na(dataToPredict[,feature]))
    if (missing > 0) {
        print(c(feature,missing))
    }
}

#########################################################################################

#Removing features having huge number of missing values
sum(is.na(dataToPredict$featurename))
dataToPredict$featurename = NULL # Number of missing values for featurename = #####

#Checking the number of missing values left after removing the above feature/features
sum(is.na(dataToPredict))


##########################################################################################

#Imputting remanining values using KNNimputation
library(DMwR)
# 
dataToPredict_no_NA<-knnImputation(dataToPredict)
#View(dataToPredict_no_NA)

#Checking the number of missing values left after KNN imputation
sum(is.na(dataToPredict_no_NA))
dim(dataToPredict_no_NA)

#########################################################################################

#Saving a CSV file of imputed data to avoid imputation time while re-running the code  
#write.csv(dataToPredict_no_NA, "dataToPredict_no_NA_new.csv", row.names=F)

#dataToPredict_no_NA = read.csv("dataToPredict_no_NA_new.csv", header = T,sep = ",")
```


## Exploratory Data Analysis - 
###Scatterplot
```{r }

pairs(Train_Data_no_NA)

library(ggplot2)
ggplot(Train_Data_no_NA, aes(colName1, colName2, color = colName3(catrgoricalVariable))) +
geom_point(shape = 16, size = 5, show.legend = TRUE) 
```
###Covariance
```{r}
cov(cars$Weight,cars$Price)
```
###Correlation
```{r}
cor_data = cor(cars[,c(1,6)])

library(corrplot)
corrplot(cor_data, method = "number")

```

### Dummyify categorical variables.

```{r}
Train_Data_no_NA$colName3 = dummy(Train_Data_no_NA$colName3)[,1]

#install.packages("dummies")
library(dummies)
DummyVars<-dummy(Train_Data_no_NA$colName3)
View(Train_Data_no_NA)
head(DummyVars)

#adding to original table
Train_Data_no_NA<-data.frame(Train_Data_no_NA,DummyVars)
Train_Data_no_NA

#Removing the original column from new dataframe
Train_Data_no_NA$colName3 = NULL
```


###Transformations
```{r}
#Visualize the data 
pairs(Boston_Housing_copy)

#OR by using histograms

#install.packages("tidyr")
#install.packages("purrr")
library(tidyr)
library(purrr)

Boston_Housing_copy %>%
  purrr::keep(is.numeric) %>%
  tidyr::gather() %>%
  ggplot2::ggplot(aes(value))+
  facet_wrap(~key, scales='free')+
  geom_histogram()

Boston_Housing %>%
  purrr::keep(is.numeric) %>%
  tidyr::gather() %>%
  ggplot2::ggplot(aes(value))+
  facet_wrap(~key, scales='free')+
  geom_histogram()

#find_skewness(df)

#Create a safe copy of the dataset 
# Bostan_Data_transform = Boston_Housing_copy[names(Boston_Housing_copy) %in% c('AGE', 'CHAS',
#                         'B','CRIM','INDUS','LSTAT','MV','NOX','PT','RAD1', 'RAD2','RAD3',
#                         'RAD4','RAD5','RAD6','RAD7','RAD8','ZN')]

#OR

#Bostan_Data_transform = Boston_Housing_copy

#Guidelines 
  # If moderately +ve skewness (Right Skewness) : sqrt(X)
        # Bostan_Data_transform$AGE   = sqrt(Bostan_Data_transform$AGE)
  # If substantially +ve skewness (Right Skewness) : log10(X)
        # Bostan_Data_transform$CRIM = log10(Bostan_Data_transform$CRIM)
  # If substantially +ve skewness with zero values (Right Skewness) : log10(X+C)
        # Bostan_Data_transform$ZN = log10(1+Bostan_Data_transform$ZN)
  # If moderately -ve skewness (Left Skewness) : sqrt(K - X)                    ............. K +a constant from which each score is substracted. Usually (largest score+1)
        # Bostan_Data_transform$AGE   = sqrt(23-Bostan_Data_transform$PT)
  # If substantially -ve skewness (Left Skewness) : log10(K - X)
        # Bostan_Data_transform$CRIM = log10(23-Bostan_Data_transform$PT)  

```


##Binning
**Discretization**
```{r}

#Example
# k=c(1,4,9,10,14,15,21,22,23,24,25)
# kBin <- discretize(k, disc="equalfreq",nbins=4)
# kBin
# summary(kBin)


library(infotheo)
colName_bin=discretize(Train_Data_no_NA$colName,disc="equalwidth",nbins=4)       #For splitting the data in 4 equal width (max-min)/n where n is number of bins
table(colName_bin)

#### OR #####

colName_bin <- discretize(Train_Data_no_NA$colName, disc="equalfreq",nbins=4)             #the bins are designed in a way such that the same or nearly the same number of continuous data points are allocated to each discrete data group. Used when data is concentrated in a single or some bins

summary(colName_bin)

```

**Standardization**
```{r}
library(vegan)
# k=c(2,4,5,6,7,20)
# n <- decostand(k,"range") # using range method
# n
# s <- decostand(k,"standardize")     # using z score
# n
# s

#Do not scale ID and target columns
Train_Data_no_NA_scaled =decostand(Train_Data_no_NA[,1:63], method = "range")
  
```

**Aggregation**
```{r}
# DF = data.frame(x=c(1,2,3,4,5,6,7,8,9,10), y=c(10,4,6,4,10,6,8,8,4,10))
# y_level<-aggregate(x~y,data = DF,FUN=sum)
# head(y_level)


```

##Subsetting numerical and categorical data
```{r}
Train_Data_no_NA_NumAtr<-subset(Train_Data_no_NA,select=c(colnam1,colnam2,colnam3))
Train_Data_no_NA_CatAtr<-subset(Train_Data_no_NA,select=-c(colnam1,colnam2,colnam3))

Train_Data_no_NA_CatAtr<-data.frame(apply(Train_Data_no_NA_CatAtr,2,function(x){as.factor(x)}))
str(Train_Data_no_NA_CatAtr)
```

##Validation
**Linear models: Train Test Split (70:30)**
```{r}
rows = seq(1,nrow((cars)))
trainRows = sample(rows,(70*nrow(cars))/100)
cars_train = cars[trainRows,]
cars_test = cars[-trainRows,]
dim(cars_train)
dim(cars_test)
```

## **Linear Regression**
```{r SimpleLinearRegression}

# - `lm(F=formula, data)` is the function in R to build a Linear Regression model
LinReg1 = lm(Price~Weight,data=cars_train)


#Utility function for regressing all the dependent variables one by one against the independent variable
simpleLinearRegression = function(df,y){
  independentVar = colnames(df)
  #depVar = independentVar[independentVar %in% y]
  independentVar = independentVar[!independentVar %in% y]
  for (colnam in independentVar) {
    regFormula = formula(paste(y,"~",colnam))
    print("-----------------------------------------------------------------")
    print(regFormula)
    print(summary(lm(regFormula,df)))
    plot(lm(regFormula,df), main = regFormula)
  }
}


# ## Read the model summary
# 
# - Summary displays the following: 
# * Formula given (Call) - Shows the function call used to compute the regression model.
# * Residuals. Provide a quick view of the distribution of the residuals, which by definition have a mean zero.
# * Coefficients and the test statistic values. Shows the regression beta coefficients and their statistical significance. Predictor variables, that are significantly associated to the outcome variable, are marked by stars.
# * Residual Standard Error (RSE)
# * Multiple R- Squared (which we generally refer to as R squared or Co-efficient of Determination)
# * F statistic - Test for Model
# 
# 
# - The statistical hypothesis is as follows :
# 
# * Null Hypothesis (H0): the coefficients are equal to zero (i.e., no relationship between x and y)
# * Alternative Hypothesis (Ha): the coefficients are not equal to zero (i.e., there is some relationship between x and y)

```


## **Multiple Linear Regression**
```{r}
LinReg1 = lm(Price~.,data=cars_train)
```



##Influential Points
**1. Outliers**
```{r}
model = lm(IndependentVariable~DependentVariable,df)

cook = cooks.distance(model)
cook
plot(cook,ylab="Cooks distances")

#Utility function for calculating and plotting Cook's Distance for all the datapoints in all the independent variables against the dependent variable.
cooksDist = function(df,y){
  independentVar = colnames(df)
  #depVar = independentVar[independentVar %in% y]
  independentVar = independentVar[!independentVar %in% y]
  for (colnam in independentVar) {
    regFormula = formula(paste(y,"~",colnam))
    print("-----------------------------------------------------------------")
    print(regFormula)
    cook = cooks.distance(lm(regFormula,df))
    print(cook)
    plot(cook,ylab="Cooks distances", main = regFormula)
  }
}

HighCookDist = function(cook){
  count = 0
  vect = c()
  for (i in 1:length(cook)){
    if (cook[i]>=0.5){
      sendop= "Flag the observation"
      count = count + 1
      vect = append(vect,i)
    }
    print("Total number of high Cook's Distance observations: ")
    print(count)
    print("Flag the observation at index %d:")
    #print(vect)
    return(vect)
  }
}

#If Cook's D of any observation (Di) > 1, that observation can be considered as having too much influence, but investigate values greater than 0.5 also.
#Relative size interpretation: In general, investigate any value that is very different from the rest.
```

**2. Residuals**
```{r}
model = lm(CurrentSalary~PriorSalary,salary_data_Influential)

residuals = model$residuals
outliers <- boxplot(residuals,plot=T)$out

sort(outliers)
length(outliers)

```

**3. Leverage**
```{r}
model_lev_1 = lm(MV~CRIM ,Boston_Housing)
lev1= hat(model.matrix(model_lev_1))
lev1
HighLeverageIndex(lev1)
mean(lev1)
plot(lev1)

leverage = function(df,y){
  independentVar = colnames(df)
  #depVar = independentVar[independentVar %in% y]
  independentVar = independentVar[!independentVar %in% y]
  for (colnam in independentVar) {
    regFormula = formula(paste(y,"~",colnam))
    print("-----------------------------------------------------------------")
    print(regFormula)
    lev = hat(model.matrix(lm(regFormula,df)))
    print(lev)
    plot(lev, main=regFormula)
    HighLeverageIndex(lev)
  }
}

HighLeverageIndex = function(leverage){
  count = 0
  vect = c()
  for (i in 1:length(leverage)){
    if (leverage[i]>=2*mean(leverage)){
      sendop= "Flag the observation"
      count = count + 1
      vect = append(vect,i)
    }
  print("Total number of high leverage observations: ")
  print(count)
  print("Flag the observation at index %d:")
  #print(vect)
  return(vect)
  }
}

# h = (1+z**2)/n
#The sum of leverages = # of parameters, p (regression coefficients including intercept).??? (= z*)*2+????2
#Flag observation whose h > 3* avg(h) or h > 2* avg(h)
```

## Residual Plots
```{r}
plot(salary_data_Influential$PriorSalary, salary_data_Influential$CurrentSalary, 
     main="Regression Model",
     xlab="PriorSalary",
     ylab="CurrentSalary"
)

abline(model, 
       col="steelblue",
       lty=1,
       lwd=4
)
```


##Residual Analysis
```{r}
# To extract the residuals:
head(LinReg1$residuals)

# To extract the train predictions:
head(LinReg1$fitted.values)

## Plot residuals
# * Residual is nothing but the difference between the predicted value and the actual value i.e $$y_i-/hat{y}_i$$
# - where  i is the $i^{th}$ training sample

# We can extract the residuals from the linear model and plot them.
plot(LinReg1$residuals,ylab="Residuals",main="Weight Residuals",col = 'brown', lwd = 1)
grid(10,10,lwd = 1)

## Plot residuals vs fitted values
# - This will help us visualize how the residuals are distributed in relation to the fitted values
plot(LinReg1$fitted.values,LinReg1$residuals,
     main = "Residual vs Predicted values", 
     col = 'brown',lwd = 1,
     xlab ="Predicted Values / Fitted Values", 
     ylab = "Residuals")
abline(h = 0,col = 'blue',lwd  =2)
# lines(xPlot,yPlot,lwd=2,col="blue")
grid(10,10,lwd=1)

## Residual plots with R plot function
#In R four diagnostic plots can be obtained by calling the plot function on fitted model obtained using lm
# par(mfrow = c(2,2)) # par helps us set graphical parameters, refer to help for more info on this
plot(LinReg1,lwd =1,col = 'brown')
```


## Variance Inflation Factor
```{r}
#Test for multicollinearity. 
# 1. Sign of estimated regression coefficient when interacting may be opposite of the signs when used as individual predictors.
# 2. Multicollinearity can lead to a model where the model (F value) is significant but all individual predictors (t values) are insignificant.

#Variance Inflation Factor (VIF): A regression analysis is conducted to predict an independent variable by the other independent variables.  The independent variable being predicted becomes the dependent variable in this analysis.


model <- lm(CurrentSalary~.,salary_data)
vif(model)

#VIF > 4 (????????2>0.75), 5 (????????2>0.80) and 10 (????????2>0.90) are commonly used as rules of thumb to indicate severe multicollinearity.
#In practical situations, sometimes even 1.5 is considered as large VIF.
#Remove such variables, rebuild models and compare with earlier model. Make decision based on whether accuracy of prediction is more important to the business or interpretation of the model and the coefficients.
#Blindly using the rules of thumb for VIF may be impractical.


```

## Stepwise Regression
```{r}
### 1. Backward
step <- stepAIC(lm(CurrentSalary~.,salary_data), direction="backward")
 

### 2. Forward
step <- stepAIC(lm(CurrentSalary~1,salary_data), direction="forward", scope = ~PriorExperience + PriorSalary + Gender + Holidays)
 

### 3. Both
step <- stepAIC(lm(CurrentSalary~.,salary_data), direction="both")
 
#AIC = 2k + nln(RSS/n) where RSS is Residual Sum of Squares or SSE.
#Small AIC means small SSE or RSS. This indicates that our model is good
```

##Validation
**Test Data**
```{r}
test_prediction1 = predict(LinReg1, cars_test)
test_actual1 = cars_test$Price
test_prediction1
test_actual1

plot(test_prediction1)
plot(test_actual1)
```






## SVM
### Tuning for the optimal C

Now, let's create a sampling strategy using the trainControl() function and use the train() function from the caret package to get the best value of C

_One way to tune models, is first using an exponential search space and then doing a more refined search near the optimal area_


Expand on the range that gives highest accuracy

```{r}

sampling_strategy <- trainControl(method = "repeatedcv", number = 4, repeats = 10)

SVM_linear_tune <- train(Cancer ~ . , 
                   train_data, 
                   method = "svmLinear", 
                   tuneGrid = data.frame(.C = 10^seq(-2, 0, .25)), 
                   trControl = sampling_strategy, 
                   set.seed(1234)
                   )

### OR ###

SVM_Poly_tune <- train(MV ~ . , 
                         Boston_train, 
                         method = "svmPoly", 
                         tuneGrid = expand.grid(.C = 10^seq(-1, 1,.1), 
                                               .scale = seq(0, 1, 0.25),
                                               .degree = c(2, 3, 5)), 
                         trControl = sampling_strategy, 
                         set.seed(1234)
)

### OR ###

SVM_Radial_tune <- train(MV ~ . , 
                         Boston_train, 
                         method = "svmRadial", 
                         tuneGrid = expand.grid(.C = 10^seq(-1, 1,.1), 
                                                .sigma = 10^seq(-2.5, -1, .25)), 
                         trControl = sampling_strategy, 
                         set.seed(1234)
)

SVM_linear_tune

### OR ###

SVM_Poly_tune

### OR ###

SVM_Radial_tune

```


_Hence, from the above cross validation experiment, we can choose the C parameter that gives us the best cross validation accuracy_

```{r}

SVM_linear_tune$finalModel
SVM_linear_tune$results
### OR ###

SVM_Poly_tune$finalModel
SVM_Poly_tune$results

### OR ###

SVM_Radial_tune$finalModel
SVM_Radial_tune$results

```

Predict SVM on the test data 

```{r}

pred_SVM_linear <- predict(SVM_linear_tune, test_data)

### OR ###

pred_SVM_Poly <- predict(SVM_Poly_tune, test_data)

### OR ###

pred_SVM_Radial <- predict(SVM_Radial_tune, test_data)

```

Evaluate linear SVM on the test data 

```{r}

plot(Boston_test$MV)
points(pred_SVM_linear, col="red")

library(Metrics)
rmse(Boston_test$MV, pred_SVM_linear)

confusionMatrix(pred_SVM_linear, test_data$Cancer)

```

### Understanding the importance of gamma / sigma / scale in SVM

* It defines the extent of impact of each support vector in making predictions.

* A low value of gamma means more impact and vice versa

* Larger the value of gamma / sigma / scale, larger the impact of each point and therfore less variance and more bias.

* Smaller the value of gamma / sigma / scale, smaller the impact of each point and therfore more variance and less bias.

 


##Random Forest

```{r}
library(randomForest)
#Tune mtry 
tuneRF(Boston_train, Boston_train$MV, ntreeTry = 20, stepFactor = 1, improve = 0.05)

#Plotting model
model_rf <- randomForest(MV ~ . , Boston_train,ntree = 110, mtry=4)

#Tune OOB Vs number of trees
plot(model_rf)
model_rf$ 
```

Predict RF on the test data 
```{r}

pred_RF = predict(model_rf, Boston_test[,-14])

plot(Boston_test$MV)
points(pred_RF, col="red")

library(Metrics)
rmse(Boston_test$MV, pred_RF)
```

